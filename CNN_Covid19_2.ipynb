{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VSEUNrzg-9js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276618d0-16f1-48f4-a7bd-d5d12016cae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/my\\content/gdrive/MyDrive/DBSCAN.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQYYVs7WPcWN",
        "outputId": "ed6d21a2-0cb6-41c6-b60b-ef0296baf3d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open gdrive/mycontent/gdrive/MyDrive/DBSCAN.zip, gdrive/mycontent/gdrive/MyDrive/DBSCAN.zip.zip or gdrive/mycontent/gdrive/MyDrive/DBSCAN.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_b = '/content/gdrive/MyDrive/DBSCAN.zip'"
      ],
      "metadata": {
        "id": "RfUchqhzSKot"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8WW7b-dipm0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53733b5b-8029-4e09-d0a6-792702e50f35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Deep learning libraries\n",
        "#import keras.backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "#Util Component 1: Confusion matrix report/Accuracy measures\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# disabling warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True #Jordan_note: Disable red warning lines seen at model architecture creation."
      ],
      "metadata": {
        "id": "qptZuP_YX7c3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def renderConfusionMetrics ( ___model, _testData, _testLabels, enableTraining, ___train_gen, ___test_gen, __batch_size, __epochs, hdf5_testSaveFileName ):\n",
        "    preds = ___model.predict(_testData)\n",
        "\n",
        "    acc = accuracy_score(_testLabels, np.round(preds))*100\n",
        "    cm = confusion_matrix(_testLabels, np.round(preds))\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "\n",
        "    print('\\nCONFUSION MATRIX FORMAT ------------------\\n')\n",
        "    print(\"[true positives    false positives]\")\n",
        "    print(\"[false negatives    true negatives]\\n\\n\")\n",
        "\n",
        "    print('CONFUSION MATRIX ------------------')\n",
        "    print(cm)\n",
        "\n",
        "    print('\\nTEST METRICS ----------------------')\n",
        "    precision = tp/(tp+fp)*100\n",
        "    recall = tp/(tp+fn)*100\n",
        "    specificity = tn/(tn+fp)*100 #Jordan_note: added specificity calculation \n",
        "    print('Accuracy: {}%'.format(acc))\n",
        "    print('Precision: {}%'.format(precision))\n",
        "    print('Recall/Sensitivity: {}%'.format(recall)) #Jordan_note: added sensitivity label\n",
        "    print('Specificity {}%'.format(specificity)) #Jordan_note: added specificity calculation \n",
        "    print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
        "\n",
        "\n",
        "    if enableTraining:\n",
        "        checkpoint = ModelCheckpoint(filepath=hdf5_testSaveFileName, save_best_only=True, save_weights_only=True)\n",
        "        lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
        "        early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\n",
        "\n",
        "\n",
        "        hist = ___model.fit_generator(\n",
        "                   ___train_gen, steps_per_epoch=___test_gen.samples // __batch_size, \n",
        "                   epochs=__epochs, validation_data=___test_gen, \n",
        "                   validation_steps=___test_gen.samples // __batch_size, callbacks=[checkpoint, lr_reduce])\n",
        "\n",
        "        print('\\nTRAIN METRIC ----------------------')\n",
        "        print('Covid19 Train acc: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    ax = ax.ravel()\n",
        "    for i, met in enumerate(['accuracy', 'loss']):\n",
        "        ax[i].plot(hist.history[met])\n",
        "        ax[i].plot(hist.history['val_' + met])\n",
        "        ax[i].set_title('Model {}'.format(met))\n",
        "        ax[i].set_xlabel('epochs')\n",
        "        ax[i].set_ylabel(met)\n",
        "        ax[i].legend(['train', 'val'])\n",
        "    plt.savefig('train_val_acc_loss.png')"
      ],
      "metadata": {
        "id": "7olDt8zoX7Zm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Util Component 2:model architecture description\n",
        "def defineModelArchitecture (_img_dims ):\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(_img_dims, _img_dims, 3))\n",
        "\n",
        "    # First conv block\n",
        "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Second conv block\n",
        "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Third conv block\n",
        "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Fourth conv block\n",
        "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.2)(x)\n",
        "\n",
        "    # Fifth conv block\n",
        "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.2)(x)\n",
        "\n",
        "    # FC layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=512, activation='relu')(x)\n",
        "    x = Dropout(rate=0.7)(x)\n",
        "    x = Dense(units=128, activation='relu')(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    x = Dense(units=64, activation='relu')(x)\n",
        "    x = Dropout(rate=0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(units=1, activation='sigmoid')(x)\n",
        "    \n",
        "    return inputs, output\n",
        "\n",
        "\n",
        "###########\n",
        "#Util Component 3: Data processor\n",
        "#Note: This process does not use validation path, because validation path in the original competion reasonably had too little data (8 samples) to create any insight.\n",
        "# the \"directoryProcessArray\" param from \"reportFileDistributions\" function corresponds to the hard-coded sub-directories of train and test, excluding val.\n",
        "def process_data(___inputPath, img_dims, batch_size):\n",
        "    # Data generation objects\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, vertical_flip=True)\n",
        "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # This is fed to the network in the specified batch sizes and image dimensions\n",
        "    train_gen = train_datagen.flow_from_directory(\n",
        "    directory=___inputPath+'train', \n",
        "    target_size=(img_dims, img_dims), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary', \n",
        "    shuffle=True)\n",
        "\n",
        "    test_gen = test_val_datagen.flow_from_directory(\n",
        "    directory=___inputPath+'test', \n",
        "    target_size=(img_dims, img_dims), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary', \n",
        "    shuffle=True)\n",
        "    \n",
        "    # I will be making predictions off of the test set in one batch size\n",
        "    # This is useful to be able to get the confusion matrix\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "\n",
        "    for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
        "        for img in (os.listdir(___inputPath + 'test' + cond)):\n",
        "            img = cv2.imread(___inputPath+'test'+cond+img,0) #Replace plt.imread, with  gray scale cv2.imread(path,0), so that ui's image load process doesn't throw a pyimage10 error\n",
        "            img = cv2.resize(img, (img_dims, img_dims))\n",
        "            img = np.dstack([img, img, img])\n",
        "            img = img.astype('float32') / 255\n",
        "            if cond=='/NORMAL/':\n",
        "                label = 0\n",
        "            elif cond=='/PNEUMONIA/':\n",
        "                label = 1\n",
        "            test_data.append(img)\n",
        "            test_labels.append(label)\n",
        "        \n",
        "    test_data = np.array(test_data)\n",
        "    test_labels = np.array(test_labels)\n",
        "    \n",
        "    return train_gen, test_gen, test_data, test_labels\n",
        "    \n",
        "\n",
        "###########\n",
        "#Util Component 4: Report file distributions\n",
        "#directoryProcessArray eg, = ['train', 'val', 'test'], in the case that training val and test folders exist in sub-dir for processing.\n",
        "def reportFileDistributions (___inputPath, directoryProcessArray):\n",
        "    for _set in directoryProcessArray:\n",
        "        n_normal = len(os.listdir(___inputPath + _set + '/NORMAL'))\n",
        "        n_infect = len(os.listdir(___inputPath + _set + '/PNEUMONIA'))\n",
        "        print('Set: {}, normal images: {}, regular pneumonia images: {}'.format(_set, n_normal, n_infect))"
      ],
      "metadata": {
        "id": "nATqQD5uX7Wg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seeds for reproducibility\n",
        "seed = 232\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "jFVdw2srX7S9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "img_dims = 150\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "qR-T94SCX7QO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, output = defineModelArchitecture ( img_dims )"
      ],
      "metadata": {
        "id": "UwzhVpDWX7Mo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating model and compiling\n",
        "model_pneumoniaDetector = Model(inputs=inputs, outputs=output)\n",
        "model_pneumoniaDetector.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_pneumoniaDetector.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r_GF8c8X7Is",
        "outputId": "4305a4ca-97e1-4e8d-be2a-b4d34452573d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 150, 150, 16)      448       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 150, 150, 16)      2320      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 75, 75, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " separable_conv2d (Separable  (None, 75, 75, 32)       688       \n",
            " Conv2D)                                                         \n",
            "                                                                 \n",
            " separable_conv2d_1 (Separab  (None, 75, 75, 32)       1344      \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 75, 75, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 37, 37, 32)       0         \n",
            " 2D)                                                             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_b = '/content/gdrive/MyDrive/DBSCAN.zip'"
      ],
      "metadata": {
        "id": "wJlBfHr1X6sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report file distributions\n",
        "reportFileDistributions (input_path_b, ['train', 'val', 'test'] )"
      ],
      "metadata": {
        "id": "8dgW8DLAY0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the data\n",
        "train_gen, test_gen, test_data_b, test_labels_b = process_data(input_path_b, img_dims, batch_size)"
      ],
      "metadata": {
        "id": "sc3Ew4tcY0pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "renderConfusionMetrics(model_pneumoniaDetector, test_data_b, test_labels_b, True, train_gen, test_gen, batch_size, 60, 'model_weights.hdf5')"
      ],
      "metadata": {
        "id": "Cwjqyn4UY0l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, output = defineModelArchitecture(img_dims)\n",
        "\n",
        "# Creating model and compiling\n",
        "model_covid19PneumoniaDetector = Model(inputs=inputs, outputs=output)\n",
        "model_covid19PneumoniaDetector.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_covid19PneumoniaDetector.load_weights('model_weights.hdf5')"
      ],
      "metadata": {
        "id": "DdKtpKutY0iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_d = '/content/covid19_DCGAN'"
      ],
      "metadata": {
        "id": "ztt1NkI7Y0en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reportFileDistributions (input_path_d, ['train', 'test'])"
      ],
      "metadata": {
        "id": "1Xfi8QMgY0Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen_d, test_gen_d, test_data_d, test_labels_d = process_data(input_path_d, img_dims, batch_size)"
      ],
      "metadata": {
        "id": "uWQn0zyiY0NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "renderConfusionMetrics(model_covid19PneumoniaDetector, test_data_d, test_labels_d, True, train_gen_d, test_gen_d, batch_size, 10, 'covid19_model_weights.hdf5')"
      ],
      "metadata": {
        "id": "TXmlsBJQZJoo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}